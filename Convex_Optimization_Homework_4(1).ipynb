{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba60788",
   "metadata": {},
   "source": [
    "# Gradient descent and Newton's method\n",
    "\n",
    "(This exercise originates from S. Boyd's course Convex Optimization)\n",
    "\n",
    "Consider the unconstrained problem\n",
    "  \n",
    "  $$\\begin{array}{ll}minimize\\,\\,\\, f(x) = - \\sum_{i=1}^m \\log(1-a_i^T x)- \\sum_{i=1}^n \\log(1 - x_i^2),\\end{array}$$\n",
    "  \n",
    "  with variable $x \\in \\mathbf{R}^n$, and $\\mathbf{dom} f = \\{x \\;|\\; a_i^Tx \\le 1, ~i=1, \\ldots, m, ~|x_i| \\le 1, ~i=1, \\ldots, n\\}$. This is the problem of computing the analytic center of the set of linear inequalities\n",
    "  \n",
    "  $$a_i^Tx \\leq 1, \\quad i=1, \\ldots, m, \\qquad |x_i|\\leq 1,\\quad i=1, \\ldots, n.$$\n",
    " \n",
    " Note that we can choose $x^{(0)}=0$ as our initial point. You can generate instances of this problem by choosing $a_i$ from some distribution on $\\mathbf{R}^n$.\n",
    " \n",
    "\n",
    "1. Use the gradient descent method to solve the problem. Use backtracking line search (BLS) to choose the step size and a stopping criterion of the form $\\| \\nabla f(x)\\|_2^2 \\leq \\delta$. You need to choose the parameters $\\alpha,\\beta$ of BLS as well as $\\delta$ in a reasonable way. Plot the objective function and step length versus iteration number. (Once you have determined $p^\\star$ to high accuracy, you can also plot $f-p^\\star$ versus iteration.) Experiment with the backtracking parameters $\\alpha$ and $\\beta$ to see their effect on the total number of iterations required. \n",
    "2. Repeat using Newton&#39;s method, again with BLS and this time with a stopping criterion based on the square $\\lambda^2$ of Newton decrement. Produce the plots as for GD. Can you see the sharp distinction between the areas of slow and fast convergence for the Newton&#39;s method?\n",
    "\n",
    "Some notes and tips:\n",
    "* You can find the description of BLS in the slides for the Newton's method lecture. The exact same formulation can be used for GD.\n",
    "* The domain of the objective is not $\\mathbf{R}^n$. So, your BLS needs make sure that the chosen step length results in a point inside the domain (on top of satisfying the standard BLS condition). \n",
    "* You are going to have to find formulas for $\\nabla f(x)$ and $\\nabla^2 f(x)$ by hand using chain rule. If you really want to, you can use pytorch (or other similar tools) for this task, but it is probably not worth it. The GD code and the Newton code has to be your own.\n",
    "* In the Newton's algorithm do not invert the Hessian, use ``numpy.linalg.solve`` (or other linear system solver) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "327a1eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cbb803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_1_minus_dot_prod(ai, x, n):\n",
    "    dot_prod = 0\n",
    "    for i in range(n):\n",
    "        dot_prod += ai[i] * x[i]\n",
    "    return math.log(1 - dot_prod)\n",
    "\n",
    "def f(x, A, n, m):\n",
    "    msum = 0\n",
    "    for i in range(m):\n",
    "        msum += log_1_minus_dot_prod(A[i], x, n)\n",
    "    nsum = 0\n",
    "    for i in range(n):\n",
    "        nsum += math.log(1 - x[i] ** 2)\n",
    "    return 0 - msum - nsum\n",
    "\n",
    "def deriviative_f(x, A, n, m):\n",
    "    ret = []\n",
    "    for j in range(n):\n",
    "        sum = 0\n",
    "        for i in range(m):\n",
    "            sum += A[i][j] / (1 - A[i][j] * x[j])\n",
    "        sum += 2 * x[j] / (1 - x[j] ** 2)\n",
    "        ret.append(sum)\n",
    "    return ret\n",
    "\n",
    "def hessian_f(x, A, n, m):\n",
    "    # define the hessian matrix as a 2D array of 0\n",
    "    hessian = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        sum = 0\n",
    "        for j in range(m):\n",
    "            sum += 2 * A[j][i] ** 2 / (1 - A[j][i] * x[i]) ** 2\n",
    "        sum += 2 / (1 - x[i] ** 2) ** 2\n",
    "        hessian[i][i] = sum\n",
    "    return hessian\n",
    "\n",
    "def dot_product(x, y):\n",
    "    dot_prod = 0\n",
    "    for i in range(len(x)):\n",
    "        dot_prod += x[i] * y[i]\n",
    "    return dot_prod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a9bc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_A(n, m):\n",
    "    A = []\n",
    "    for i in range(m):\n",
    "        ai = []\n",
    "        for j in range(n):\n",
    "            ai.append(rand.uniform(-1, 1))\n",
    "        A.append(ai)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "082dc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_domain(x, A, n, m) -> bool:\n",
    "    for i in range(n):\n",
    "        if x[i] >= 1 or x[i] <= -1:\n",
    "            return False\n",
    "    for i in range(m):\n",
    "        if dot_product(A[i], x) >= 1:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99bfa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLS(alpha, beta, x, A, n, m, direction):\n",
    "    ni = 1\n",
    "    while True:\n",
    "        new_x = [x[i] + ni * direction[i] for i in range(n)]\n",
    "        if in_domain(new_x, A, n, m) and f(new_x, A, n, m) < f(x, A, n, m) + alpha * ni * dot_product(deriviative_f(x, A, n, m),direction):\n",
    "            return ni\n",
    "        ni *= beta\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5abfd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, A, n, m, alfa = 0.25, beta = 0.5):\n",
    "    objective_function_vals = []\n",
    "    step_lenghts = []\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        direction = deriviative_f(x, A, n, m)\n",
    "        direction = [0 - i for i in direction]\n",
    "        #direction = [i / abs(i) for i in direction]\n",
    "        ni = BLS(alfa, beta, x, A, n, m, direction)\n",
    "        direction = [i * ni for i in direction] \n",
    "        x = [x[i] + direction[i] for i in range(n)]\n",
    "        objective_function_vals.append(f(x, A, n, m))\n",
    "        step_lenghts.append(ni)\n",
    "        iteration += 1\n",
    "        print(abs(f(x, A, n, m) - f([x[i] - direction[i] for i in range(n)], A, n, m)))\n",
    "        if abs(f(x, A, n, m) - f([x[i] - direction[i] for i in range(n)], A, n, m)) < 0.001:\n",
    "            return x, objective_function_vals, step_lenghts, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42552fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newtons_Method(x, A, n, m, alfa = 0.25, beta = 0.5):\n",
    "    objective_function_vals = []\n",
    "    step_lengths = []\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        gradient = deriviative_f(x, A, n, m)\n",
    "        hessian = hessian_f(x, A, n, m)\n",
    "        direction = np.linalg.solve(hessian, gradient)\n",
    "        ni = BLS(alfa, beta, x, A, n, m, direction)\n",
    "        direction = [i * ni for i in direction]\n",
    "        x = [x[i] + direction[i] for i in range(n)]\n",
    "        objective_function_vals.append(f(x, A, n, m))\n",
    "        step_lengths.append(ni)\n",
    "        if np.linalg.norm(gradient) ** 2 <= 0.0001:\n",
    "            return x, objective_function_vals, step_lengths, iteration\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c84384",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 0, 0, 0, 0]\n",
    "best_ofv = []\n",
    "best_step = []\n",
    "best_x = []\n",
    "min_iter_num = 1000\n",
    "for alpha in [0.25, 0.5, 0.75]:\n",
    "    for beta in [0.5, 0.75, 0.9]:\n",
    "        x, objective_function_vals, step_lenghts, iter_num = Newtons_Method(x, A, 5, 5, alfa=alpha, beta=beta)\n",
    "        if iter_num < min_iter_num:\n",
    "            min_iter_num = iter_num\n",
    "            best_ofv = objective_function_vals\n",
    "            best_step = step_lenghts\n",
    "            best_x = x\n",
    "            best_alpha = alpha\n",
    "            best_beta = beta\n",
    "print(\"Best alpha: \", best_alpha)\n",
    "print(\"Best beta: \", best_beta)\n",
    "print(\"Best x: \", best_x)\n",
    "# plot the objective function values against the iteration number\n",
    "plt.plot(range(len(best_ofv)), best_ofv)\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"Objective function value\")\n",
    "plt.show()\n",
    "\n",
    "# plot the step lengths against the iteration number\n",
    "plt.plot(range(len(best_step)), best_step)\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"Step length\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
